 ---
title: "Movielens Project"
author: "Derrick Owusu Ofori"
date: "11/01/2021"
output:
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
---



# Introduction

Recommendation system is a class of machine learning algorithim that offers useful and relevant suggestions to users. Recommendation systems has applications in several fields but for this project we would focus on creating a movie recommender system. We will employ the MovieLens 10M dataset complied by the Grouplens Research lab found here; https://grouplens.org/datasets/movielens/10m/ and http://files.grouplens.org/datasets/movielens/ml-10m.zip. This dataset contains roughly ten million obervations and six variables namely; userId, movieId, rating, timestamp, title and genres. 

The aim of this project is to utilize a subset of the dataset to build an algorithim that can make predictions about user ratings in a validation set. To achieve this project we will first import the data, perform some exploratory data analysis and finally compare five models to determine the best RMSE for the movie rating prediction.

# Methods

## Installing packages and Importing data

```{r, message=FALSE}
#Load Packages and libraries
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(recosystem)) install.packages("recosystem", repos = "http://cran.us.r-project.org")


library(tidyverse)
library(caret)
library(data.table)
library(recosystem)
```
MovieLens 10M dataset:
https://grouplens.org/datasets/movielens/10m/
http://files.grouplens.org/datasets/movielens/ml-10m.zip

```{r}
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 3.6 or earlier:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))
# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")
```

The dataset will now be split into a train set (edx) which we will use to train, develop and select our algorithim and a test set (validation) to test the performance of the final algorithm. The test set will be 10% of the dataset.


```{r}
# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```
## Data exploration and visualization

### Average movie rating
The figure below indicates that certain movies tend to be rated highly on average while others are rated low.

```{r}
# Average movie rating
edx %>% group_by(movieId) %>%
  summarise(avg_rating = sum(rating)/n()) %>%
  ggplot(aes(avg_rating)) +
  geom_histogram(bins=35, color ="white") +
  labs(x = "Average Rating", y = "Movies")
```

### Number of ratings per movie
This figure also shows certain movies having a lot more number of ratings than other movies. This in combination with the variance within the average movie rating all indicate a movie effect which will be accounted for during the final algorithm.

```{r}
edx %>% 
  count(movieId) %>% 
  ggplot(aes(n)) + 
  geom_histogram( bins=35, color = "white") +
  scale_x_log10() + labs(x = "Movies", y = "Number of Ratings")
 ```


### Average rating per user

Here we see from this figure that certain users rate movies more highly done others
```{r}
edx %>% group_by(userId) %>%
  summarise(avg_rating = sum(rating)/n()) %>%
  ggplot(aes(avg_rating)) +
  geom_histogram(bins=35, color = "white") +
  labs(x = "Average rating", y = "Number of users") 
```
### Number of ratings per user
Similar to the figure about the number of rating per movie, some users also tend to rate movies a lot more done others. This in addition to the average rating by users also demonstrate a user effect which will also be adjusted for in developing our recommendation system. 

```{r}
edx %>% 
  count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram( bins=35, color = "white") +
  scale_x_log10() +
  labs(x = "Users", y = "Number of ratings") 
  ```

## Modeling approach

In order to evaluate the performance of the algorthim we will utilise Root Mean Square Error (RMSE). The RMSE is the square root of the mean of the square of all the error. It can be calculated by squaring the residuals,finding the average of the residuals and taking the square root of the result. It basically measures the difference between the predicted and actual values, with a lower RMSE value being better than a higher value.

$$ RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{u,i} (\hat{y}_{u,i}-y_{u,i})^{2}} $$

### 1. Average Movie Rating Model
Our fist model will be a simple recommendation system. Here we will predit the same rating for all movies irrespective of the user and movie. A model-based approach will be utlized, which assumes the same rating for all movies and all users, with all differences explained by random variation. 

$$ Y_{u, i} = \mu + \epsilon_{u, i} $$

Here $\epsilon_{u,i}$ represents independent error sample from the same distribution centered at 0 and $\mu$ represents the true rating for all movies. We know that the estimate that minimizes the residual mean squared error is the least squares estimate of $Y_{u,i}$ , in this instance, it is basically the average of all the ratings.

```{r, echo = TRUE}
mu <- mean(edx$rating)
mu
```

With a mean rating of approximatly 3.51, we will movie forth and compute the naive RMSE
```{r, echo = TRUE}
naive_rmse <- RMSE(validation$rating, mu)
naive_rmse
```
Now we will tabulate the first RMSE results for later comparision with other models.

```{r, echo = TRUE}
rmse_results <- data_frame(method = "Average Movie Rating",
                           RMSE = naive_rmse)
rmse_results %>% knitr::kable()
```

### 2. Movie Effect Model

Some movies tend to be rated highly than others as we will see in the diagram below, thus we can improve our previous model by including a term, $b_{i}$ to represent the average rating for each movie  $i$. 
$$Y_{u, i} = \mu +b_{i}+ \epsilon_{u, i}$$

We know that the least sqaures estimate is the average of the $$Y_{u, i}$$ minus the overall mean for each movie $i$. This can be computed with:

```{r, echo = TRUE}
movie_avgs<- edx %>% 
  group_by(movieId) %>%
  summarise(b_i=mean(rating-mu))
```

```{r, echo = TRUE}
movie_avgs %>% qplot(b_i, geom ="histogram",
                     bins = 10, data = ., 
                     color = I("white"),
                     ylab = "Number of movies")
```
Some estimates seem to vary substantially tihs is because some movies are good and others are bad. Overall the histogram is left skewed indicating that more movies have negative effects. Now lets make make predictions with the inclusion of the movie effect.

```{r, echo = TRUE}
predicted_ratings <- mu +  validation %>%
  left_join(movie_avgs, by='movieId') %>%
  .$b_i
```
We will now tablulate this result with previous RMSE results for comparison

```{r, echo = TRUE}
model_1_rmse <- RMSE(predicted_ratings, validation$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie effect model",  
                                     RMSE = model_1_rmse ))
rmse_results %>% knitr::kable()
```
By adding the computed $b_{i}$ to $\mu$ and thus capturing the movie effect we can see and improvement in model, however there is still more we can do to make the recommender system even better.

### 3. Movie and user effect model

Next we have to consider how difference users vary in how they rate movies and as illustrated in the histogram below we can see variability across users, with some users highly rating all movies they watch whiles others are somewhere in the middle.

```{r, echo = TRUE}  
user_avgs%>% qplot(b_u, geom ="histogram", 
                   bins = 30, data = ., 
                   color = I("white"))
```

Therefore will will augment our model with a new term $b_{u}$ which is the user-specific effect. This makes our new model:

$$Y_{u, i} = \mu + b_{i} + b_{u} + \epsilon_{u, i}$$

We will therefore ompute $\mu$ and $b_{i}$, and estimating  $b_{u}$, as the average of $$Y_{u, i} - \mu - b_{i}$$

```{r, echo = TRUE}
user_avgs <- edx %>%
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))
```
We will make predictions with the user effect captured.

```{r, echo = TRUE}
predicted_ratings <- validation %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred
 ```
 We will save this result with previous RMSE results for comparison
 
```{r, echo = TRUE}
model_2_rmse <- RMSE(predicted_ratings, validation$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie and user effect model",  
                                     RMSE = model_2_rmse))

rmse_results %>% knitr::kable()
```
 
 
### 4. Regularizing movie and user effect

In our previous models a grevious error was overlooked. Some obsucre movies were rated by a few people leading to more uncertainty and therefore resulting in larger estimates for $b_{i}$ and $b_{u}$. These are noisy estimates and should not be trusteded in our predictions. Large errors can increase our residual mean squared error,so we would rather be conservative when we're not sure. Normally we compute standard errors and construct confidence intervals to account for different levels of uncertainty. However, when making predictions we need one number, one prediction, not an interval. For this, we require the concept of regularization. Regularization permits us to penalize large estimates that come from small sample sizes. The general idea is to add a penalty for large values of $b_{i}$ and $b_{u}$ to the sum of squares equation that we minimize. To do this we require a tunining parameter (lambda) that will minimize the RMSE. This will therefore minimize the estimates of $b_{i}$ and $b_{u}$ if the number of ratings are small. 

For the penalty we will use numbers from 0 to 10 with increaments of 0.25

```{r, echo = TRUE}
lambdas<- seq(0,10,0.25)

rmses <- sapply(lambdas, function(l){
  
  mu <- mean(edx$rating)
  
  b_i <- edx %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  b_u <- edx %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  
  predicted_ratings <- validation %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    .$pred
  
  return(RMSE(predicted_ratings, validation$rating))
})
```

Next we will plot rmses vs lambdas to select the optimal lambda     
```{r, echo = TRUE}                                        
qplot(lambdas, rmses)  
lambda <- lambdas[which.min(rmses)]
lambda
```
Here the optimal lambda is 5.25

Now we wil tabulate and store the regualarized results and compare it to the previous models.

```{r, echo = TRUE}  
rmse_results <- bind_rows(rmse_results,
data_frame(method="Regularized movie and user effect model",
RMSE = min(rmses)))
rmse_results %>% knitr::kable()
```


#5. Matrix factorization
Matrix factorization is a widely used concept in machine learning.
It is very much related to factor analysis, single value composition,
and principal component analysis. It works by decomposing the user-item interacton matric into product of two lower dimensionality rectangualr matrices. We will apply this approach in our recommender system by utilizing the recosystem library. 

First both the training and validation set will need to be organized into 3 columns: user, item (movies) and the value (rating).

```{r, echo = TRUE} 
edx_fc <- edx %>% select(movieId, userId, rating)
validation_fc <- validation %>% select(movieId, userId, rating)
```
We will then transform them into matrix format.

```{r, echo = TRUE}
edx_fc <- as.matrix(edx_fc)
validation_fc <- as.matrix(validation_fc)
```
Next we will write these datasets onto the hard disk and assign them to a train set (train_fc) and a validation set(valid_fc) in order to build the recosystem.

```{r, echo = TRUE}
write.table(edx_fc, file = "trainingset.txt", sep = " ", row.names = FALSE, 
            col.names = FALSE)

write.table(validation_fc, file = "validationset.txt", sep = " ", 
            row.names = FALSE, col.names = FALSE)
            
set.seed(76)
train_fc <- data_file("trainingset.txt")

valid_fc <- data_file("validationset.txt")
```

We will now build a recommender object (r) using the Reco() in the recosystem package. We utilize the $tune() approach for find the optimum tuning parameter.

```{r, echo = TRUE}
r = Reco()

opts <- r$tune(train_fc, opts = list(dim = c(10, 20, 30), lrate = 
                                               c(0.1,0.2), costp_l1 = 0, costq_l1 = 0, nthread = 1, niter = 10))
```
The recommender model will now be trained with $train().

```{r, echo = TRUE}
r$train(training_dataset, opts = c(opts$min, nthread = 1, niter = 20))
```

Next we will write the predictions to a tempfile on the Hard drive, make predictions on the validation set and calculate the RMSE.

```{r, echo = TRUE}

stored_predictions = tempfile()


r$predict(validation_dataset, out_file(stored_predictions))

actual_ratings <- read.table("validationset.txt", header = FALSE, sep = " ")$V3

predicted_ratings <- scan(stored_predictions)

rmse_fc <- RMSE(actual_ratings, predicted_ratings)
rmse_fc 
```
Finally we will save and tabulate the RMSE for comparison with the previous models.

```{r, echo = TRUE}
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Matrix factorization", RMSE = rmse_fc ))
rmse_results %>% knitr::kable()
```

