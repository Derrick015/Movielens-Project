 ---
title: "Movielens Project"
author: "Derrick Owusu Ofori"
date: "11/01/2021"
output:
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
---



# Introduction

Recommendation system is a class of machine learning algorithim that offers useful and relevant suggestions to users. Recommendation systems has applications in several fields but for this project we would focus on creating a movie recommender system. We will employ the MovieLens 10M dataset complied by the Grouplens Research lab found here; https://grouplens.org/datasets/movielens/10m/ and http://files.grouplens.org/datasets/movielens/ml-10m.zip. This dataset contains roughly ten million obervations and six variables namely; userId, movieId, rating, timestamp, title and genres. 

The aim of this project is to utilize a subset of the dataset to build an algorithim that can make predictions about user ratings in a validation set. In order to evaluate the performance of the algorthim we will utilise Root Mean Square Error (RMSE). The RMSE is the square root of the mean of the square of all the error. It can be calculated by squaring the residuals,finding the average of the residuals and taking the square root of the result. It basically measures the difference between the predicted and actual values, with a lower RMSE value being better than a higher value.

$$ RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{u,i} (\hat{y}_{u,i}-y_{u,i})^{2}} $$

To achieve the aim of this project we will first import the data, perform some exploratory data analysis and finally compare five models to determine the best RMSE for the movie rating prediction.

# Methods

## Installing packages and Importing data

```{r, message=FALSE}
#Load Packages and libraries
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(recosystem)) install.packages("recosystem", repos = "http://cran.us.r-project.org")


library(tidyverse)
library(caret)
library(data.table)
library(recosystem)
```
MovieLens 10M dataset:
https://grouplens.org/datasets/movielens/10m/
http://files.grouplens.org/datasets/movielens/ml-10m.zip

```{r}
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 3.6 or earlier:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))
# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")
```

The dataset will now be split into a train set (edx) which we will use to train, develop and select our algorithim and a test set (validation) to test the performance of the final algorithm. The test set will be 10% of the dataset.


```{r}
# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```
## Data exploration and visualization

###Number of rating per movie

Certian movies were rated a few times and hence emphasis would not be placed on them in the predictive algorithim
```{r, message= FALSE}
edx %>%
  count(movieId) %>%
  ggplot(aes(n)) +
  geom_histogram(bins = 25, color = "black", fill="green") +
  scale_x_log10() + xlab("Number of ratings") +
  ylab("Number of movies") + ggtitle("Number of ratings per movie")
```

### Number of rating per user
Some users tend to rate very few movies whiles others may rate a lot more movies. There is therefore a disparity in how active users are.

```{r, message= FALSE}
edx %>%
  count(userId) %>%
  ggplot(aes(n)) + geom_histogram(bins = 25, color = "black",fill="green") +
  scale_x_log10() +
  xlab("Number of ratings") + ylab("Number of users") +
  ggtitle("Number of ratings given by users")
  ```

## Modeling approach

### 1. Average Movie Rating Model
Our fist model will be a simple recommendation system. Here we will predit the same rating for all movies irrespective of the user and movie. A model-based approach will be utlized, which assumes the same rating for all movies and all users, with all differences explained by random variation. 

$$ Y_{u, i} = \mu + \epsilon_{u, i} $$

Here $\epsilon_{u,i}$ represents independent error sample from the same distribution centered at 0 and $\mu$ represents the true rating for all movies. We know that the estimate that minimizes the residual mean squared error is the least squares estimate of $Y_{u,i}$ , in this instance, it is basically the average of all the ratings.

```{r, echo = TRUE}
mu <- mean(edx$rating)
mu
```

With a mean rating of approximatly 3.51, we will movie forth and compute the naive RMSE
```{r, echo = TRUE}
naive_rmse <- RMSE(validation$rating, mu)
naive_rmse
```
Now we will tabulate the first RMSE results for later comparision with other models.

```{r, echo = TRUE}
rmse_results <- data_frame(method = "Average Movie Rating",
                           RMSE = naive_rmse)

rmse_results %>% knitr::kable()
```

### 2. Movie Effect Model

Some movies tend to be rated highly than others as we will see in the diagram below, thus we can improve our previous model by including a term, $b_{i}$ to represent the average rating for each movie  $i$. 
$$Y_{u, i} = \mu +b_{i}+ \epsilon_{u, i}$$

We know that the least sqaures estimate is the average of the $$Y_{u, i}$$ minus the overall mean for each movie $i$. This can be computed with:

```{r, echo = TRUE}
movie_avgs<- edx %>% 
  group_by(movieId) %>%
  summarise(b_i=mean(rating-mu))
```

```{r, echo = TRUE}
movie_avgs %>% qplot(b_i, geom ="histogram", bins = 10, data = ., color = I("black"), ylab = "Number of movies")
```
Some estimates seem to vary substantially tihs is because some movies are good and others are bad. Overall the histogram is left skewed indicating that more movies have negative effects. Now lets make make predictions with the inclusion of the movie effect.

```{r, echo = TRUE}
predicted_ratings <- mu +  validation %>%
  left_join(movie_avgs, by='movieId') %>%
  .$b_i
model_1_rmse <- RMSE(predicted_ratings, validation$rating)
```
We will now tablulate this result with previous RMSE results for comparison

```{r, echo = TRUE}
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie effect model",  
                                     RMSE = model_1_rmse ))
rmse_results %>% knitr::kable()
```
By adding the computed $b_{i}$ to $\mu$ and thus capturing the movie effect we can see and improvement in model, however there is still more we can do to make the recommender system even better.

### 3. Movie and user effect model

Next we have to consider how difference users vary in how they rate movies and as illustrated in the histogram below we can see variability across users, with some users highly rating all movies they watch whiles others are somewhere in the middle.

```{r, echo = TRUE}  
 user_avgs%>% qplot(b_u, geom ="histogram", bins = 30, data = ., color = I("black"))
```

Therefore will will augment our model with a new term $b_{u}$ which is the user-specific effect. This makes our new model:

$$Y_{u, i} = \mu + b_{i} + b_{u} + \epsilon_{u, i}$$

We will therefore ompute $\mu$ and $b_{i}$, and estimating  $b_{u}$, as the average of $$Y_{u, i} - \mu - b_{i}$$

```{r, echo = TRUE}
user_avgs <- test_set %>% 
     left_join(movie_avgs, by='movieId') %>%
     group_by(userId) %>%
     summarize(b_u = mean(rating - mu - b_i))
```
We will make predictions with the user effect captured.

```{r, echo = TRUE}
predicted_ratings <- validation%>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred
 ```
 We will save this result with previous RMSE results for comparison
 
```{r, echo = TRUE}
model_2_rmse <- RMSE(predicted_ratings, validation$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie and user effect model",  
                                     RMSE = model_2_rmse))

rmse_results %>% knitr::kable()
```
 
 
### 4. Regularizing movie and user effect

In our previous models a grevious error was overlooked. Some obsucre movies were rated by a few people leading to more uncertainty and therefore resulting in larger estimates for $b_{i}$ and $b_{u}$. These are noisy estimates and should not be trusteded in our predictions. Large errors can increase our residual mean squared error,so we would rather be conservative when we're not sure. Normally we compute standard errors and construct confidence intervals to account for different levels of uncertainty. However, when making predictions we need one number, one prediction, not an interval. For this, we require the concept of regularization. Regularization permits us to penalize large estimates that come from small sample sizes. The general idea is to add a penalty for large values of $b_{i}$ and $b_{u}$ to the sum of squares equation that we minimize. To do this we require a tunining parameter (lambda) that will minimize the RMSE. This will therefore minimize the estimates of $b_{i}$ and $b_{u}$ if the number of ratings are small. 

For the penalty we will use numbers from 0 to 10 with increaments of 0.25

```{r, echo = TRUE}
lambdas<- seq(0,10,0.25)

rmses <- sapply(lambdas, function(l){
  
  mu <- mean(edx$rating)
  
  b_i <- edx %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  b_u <- edx %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  
  predicted_ratings <- 
    validation %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    .$pred
  
  return(RMSE(predicted_ratings, validation$rating))
})
```

Next we will plot rmses vs lambdas to select the optimal lambda     

```{r, echo = TRUE}                                        
qplot(lambdas, rmses)  

lambda <- lambdas[which.min(rmses)]
lambda
```
Here the optimal lambda is 5.25

Now we wil tabulate and store the regualarized results and compare it to the previous models.

```{r, echo = TRUE}  
rmse_results <- bind_rows(rmse_results,
data_frame(method="Regularized movie and user effect model",
RMSE = min(rmses)))
rmse_results %>% knitr::kable()
```




